[{"title":"机器学习算法总结一——感知机","date":"2018-05-13T11:33:56.000Z","path":"2018/05/13/机器学习算法总结一——感知机/","text":"关键词：感知机 对偶形式 判别模型 1 感知机模型感知机（perceptron）是二类分类的简单线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1为二值，感知机对应于输入空间（特征空间）中将实例分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机模型的数学表达式如下:$$f(x)=sign(w \\cdot x+b)$$其中，w和为感知机模型参数，$w$叫作权值，b叫作偏置，$w \\cdot x$表示$w$和$x$的内积,sign是符号函数，即$$sign(x)= \\begin{cases} -1&amp; {x&lt;0}\\\\ 1&amp; {x\\geq 0} \\end{cases}$$ 感知机模型的假设空间是定义在特征空间中的所有线性分类模型，即函数集合$$\\{f|f(x)=w \\cdot x +b\\}$$这样在学习的过程中通过对训练数据集的学习确定参数w和b，然后通过学习到的感知机模型去预测新的输入实例。 2 感知机学习策略为了找出分离超平面，我们需要确定感知机模型参数$w$和b，所以需要一个有效的学习策略，即定义损失函数并将其极小化。那么很容易想到的一个损失函数的选择是误分类点的总数，但是，这样的参数不是$w$和b的连续可导函数，不容易优化。感知机学习实际上采用的是误分类点到超平面S的总距离来作为损失函数，定义如下：$$\\frac{1}{||w||} \\left|w \\cdot x_0 + b \\right|$$这里$||w||$是$w$的$L_2$范数。其次，对于误分类的数据$-y_i(w \\cdot x_i +b)&gt;0$成立，因为当$w \\cdot x_i +b &gt;0$时，$y_i=-1$,而$w \\cdot x_i +b&lt;0$时，$y_i=+1$，因此，误分类点$x_i$到超平面S的距离是$$-\\frac{1}{||w||} y_i \\left(w \\cdot x_i + b \\right)$$这样，假设误分类点的集合为M，那么总距离为：$$-\\frac{1}{||w||}\\sum\\limits_{x_i \\in M} y_i \\left(w \\cdot x_i + b \\right)$$这样我们就得到了初步的感知机模型的损失函数。我们研究可以发现，分子和分母都含有w,当分子的w扩大N倍时，分母的$L_2$范数也会扩大N倍。也就是说，可以不考虑$\\frac{1}{||w||}$。最终感知机模型的损失函数简化为：$$L(w,b) = -\\sum\\limits_{x_i \\in M} y_i \\left(w \\cdot x_i + b \\right)$$显然，损失函数$L（w,b）$是非负的，如果分类正确，即没有误分类点，则损失函数值为0，而且，误分类点聚类分类超平面越近，损失函数值越小。 3 感知机学习算法3.1 感知机学习算法的原始形式用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,可以采用随机梯度下降来学习模型。在任意选定一个超平面后，我们每次仅仅需要随机选取一个误分类的点来更新梯度。损失函数基于w和b参数的偏导数分别为：$$\\begin{aligned}\\nabla_w L(w,b) &amp;= -\\sum\\limits_{x_i \\in M} y_i x_i\\\\\\nabla_b L(w,b) &amp;= -\\sum\\limits_{x_i \\in M} y_i\\end{aligned}$$w,b的梯度下降迭代公式应该为：$$\\begin{aligned}w &amp;= w + \\eta y_i x_i\\\\\\b &amp;= b + \\eta y_i\\end{aligned}$$其中，$\\eta$叫作学习率（learning rate）。这样通过迭代可以让损失函数不断减小，直到为0。那么算法的步骤总结如下：(1) 选取初值$w_0,b_0$,一般取为0(2) 在训练集中选取数据$(x_i,y_i)$(3) 如果$y_i(w \\cdot x_i + b)\\leq 0$,$$\\begin{align}w &amp;= w + \\eta y_ix_i \\\\\\b &amp;= b + \\eta y_i\\end{align}$$(4) 转至（2），直至训练集中没有误分类点。下面将偏置b并入权重向量w, 记作$\\hat{w}=(w^T,b)^T$,相应的，$\\hat{x}=(x^T,1)^T$。显然$\\hat{w}\\cdot \\hat{x} = w \\cdot x +b$. 3.2 感知机学习算法的对偶形式对偶形式的主要目的是简化计算，其基本思想是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得参数。我们假设初始值均为0，误分类样本点$(x_i,y_i)$在更新过程中修改了n次，那么由上述的学习过程不难得到：$$\\begin{align}w &amp;= \\sum_{i=1}^Nn_i \\eta y_ix_i \\\\b &amp;= \\sum_{i=1}^Nn_i\\eta y_i\\end{align}$$当$\\eta=1$时，表示第$i$个实例点由于误分而进行更新的次数，实例点更新次数越多，意味着它距离分离超平面越近，在SVM中这样的点也越有可能是支持向量。将上式带入原始形式，令$\\alpha_i = n_i\\eta$, 则新的感知机模型为$$f(x) = sign\\{\\sum_{j=1}^N\\alpha_j y_jx_jx+b \\}.$$类比原始形式的感知机学习算法，对偶形式算法如下：(1) $\\alpha=0, b=0$(2) 在训练集中选取数据$(x_i,y_i)$(3) 如果$y_i(\\sum_{j=1}^N\\alpha_j y_jx_jx+b)\\leq 0$,$$\\begin{align}\\alpha_i &amp;= \\alpha_i+ \\eta \\\\b &amp;= b + \\eta y_i\\end{align}$$(4) 转至（2），直至训练集中没有误分类点。对偶形式的意义在于在对偶形式中训练实例仅以内积的形式出现，所以如果提前将训练集中实例间的内积计算出来并以矩阵的形式存储，将大大加快计算速率。这个矩阵就是所谓的Gram矩阵。$$G = [x_i \\cdot x_j]_{N\\times N}$$ 4 感知机的简单实现下面给出了感知机的简单python实现，分别实现了原始形式和对偶形式的感知机模型。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import numpy as npclass Perceptron: \"\"\" Perceptron classifier. Parameters: ----------- learning_rate: int \"\"\" def __init__(self, learning_rate=1): self.learning_rate = learning_rate # 原始形式 def fit(self, X, y): n_samples, n_features = np.shape(X) self.W = np.zeros(n_features) self.w0 = 0 flag = True while flag: flag = False for i in range(n_samples): # Calculate outputs linear_output = X[i].dot(self.W) + self.w0 if y[i]*linear_output &lt;= 0: flag = True self.W += self.learning_rate*y[i]*X[i] self.w0 += self.learning_rate*y[i] print(self.W, self.w0) # 对偶形式 def fit_dual(self, X, y): n_samples, n_features = np.shape(X) a = np.zeros(n_samples) b = 0 matrix = self.make_gram_matrix(X, n_samples) flag = True while flag: flag = False for i in range(n_samples): # Calculate outputs linear_output = sum([a[j] * y[j] * matrix[j][i] for j in range(n_samples)]) + b if y[i]*linear_output &lt;= 0: flag = True a[i] += self.learning_rate b += self.learning_rate*y[i] print(a,b) self.W = sum([a[i]*y[i]*X[i] for i in range(n_samples)]) self.w0 = sum([a[i]*y[i] for i in range(n_samples)]) @staticmethod def make_gram_matrix(X, n_features): matrix = np.array([X[i].dot(X[j]) for i in range(n_features) for j in range(n_features)]).reshape(n_features, n_features) return matrix # Use the trained model to predict labels of X def predict(self, x): print(\"y = &#123;&#125;x + &#123;&#125;\".format(self.W, self.w0)) y_pred = (x.dot(self.W) + self.w0) return 1 if y_pred &gt; 0 else 0 if __name__ == '__main__': per = Perceptron() train_x = np.array([[3, 3], [4, 3], [1, 1]]) train_y = np.array([1, 1, -1]) per.fit(train_x, train_y) print(per.predict(np.array([2, 3]))) per.fit_dual(train_x, train_y) print(per.predict(np.array([2, 3]))) 参考文献李航 《统计学习方法》 第二章","tags":[{"name":"算法","slug":"算法","permalink":"http://tpioneer.github.io/tags/算法/"},{"name":"笔记","slug":"笔记","permalink":"http://tpioneer.github.io/tags/笔记/"}]},{"title":"HEXO搭建你的专属博客","date":"2018-05-11T15:33:56.000Z","path":"2018/05/11/HEXO搭建你的专属博客/","text":"关键词：github hexo blog STEP1 前期准备 在这里默认你已经拥有了github账号，如果没有，还等什么，赶快加入最大的同性交友平台吧（雾。 在github中创建一个名为username.github.io（这里的username即你的账号名，比如我的为Tpioneer.github.io）的库，创建完 后可以试着往库中push一个简单的index.html如下。（github的简单操作可以参考如下教程） &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;h1&gt;Hello World&lt;/h1&gt; &lt;p&gt;I'm hosted with GitHub Pages.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 接着在你的浏览器中输入https://username.github.io/（username为你的账号名），如果没什么问题的话即可访问到Hello World。 同时，需要安装git，可以去官网下载。 STEP2 hexo环境配置与安装 hexo是一款基于Node.js的静态博客框架, 在安装hexo之前，我们需要先安装node.js,这里对安装不做过多介绍，官网的文档已经很详细了。 Node和Git都安装好后,首先创建一个blog文件夹,然后进入blog目录来安装Hexo（windows用户推荐用Git Bash）。执行如下命令安装Hexo： sudo npm install -g hexo 安装完成后，执行init命令初始化hexo,命令： hexo init然后等待安装完成，即完成了hexo的安装。 安装完成后需要对hexo进行配置，找到/hexo/_config.yml文件，用任意编辑器打开该文件，拉到最后找到Deployment,将其改为下图所示内容： 这里有个需要注意的点，在冒号后面一定要空格，否则识别不出来 修改头像，同样需要打开_config.yml ，找到 avatar: 这一行，然后添加头像的URL就行了，当然你也可以将头像图片放入hexo/source/img目录下，然后将配置改为如下：avatar: /img/xxx.jpg 同理可以修改你的博客名，博客描述，作者等信息，这里发一张我的参考配置如下： 当然配置文件中还有很多其他的配置选项，在这里就不一一赘述了，官网上提供了更详细的参考资料 STEP3 hexo的部署和常用命令 部署步骤如下(建议在每次重新部署时都执行一次hexo clean)：hexo cleanhexo generate (hexo g)hexo deploy (hexo d)在部署完成后，在你的浏览器中输入https://username.github.io/，大功告成，撒花★,°:.☆(￣▽￣)/$:.°★ 。 当然，这只是简单的部署了属于你自己的一个blog，你还需要稍微再花费点时间让你的blog更加丰富多彩。 首先，你需要了解下面提供的一些常用命令： hexo new “postName” #新建文章 hexo new page”pageName” #新建页面 hexo generate #生成静态页面至public目录 hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server） hexo deploy #将.deploy目录部署到GitHub hexo help #查看帮助 hexo version #查看Hexo的版本 了解了这些命令，相信你写一些简单的blog已经没有什么问题了，但是中庸的官方主题肯定不会让你特别喜欢。这里有个主题集合,你可以在这里尽情挑选你喜欢的主题，每个主题的github中都有相应的文档，所以，大胆的去尝试吧，相信你一定能找到适合你的那个。 最后，送上一份Markdown的语法说明书。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://tpioneer.github.io/tags/随笔/"}]}]