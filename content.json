[{"title":"机器学习总结三——K近邻","date":"2018-06-03T12:10:24.000Z","path":"2018/06/03/2018-06-03-机器学习总结三——K近邻/","text":"关键词： 1 K近邻定义k近邻(k nearest neighbors)算法是一种很简单但很有效的分类算法，这里给出一个最朴素的K近邻算法： K近邻算法输入：给出训练数据集$T=(x_1,y_1),(x_2,y_2),…(x_N,y_N)$与需预测的$x$和k值输出：实例$x$所属的类y算法步骤：(1) 根据给定的距离度量，在训练集T中找出与$x$最近邻的k个点，涵盖这k个点的$x$的邻域记作$N_k(x)$(2) 在$N_k(x)$中根据分类决策规则，如多数表决决定$x$的类别y。 2 参数的选择这里给出闵可夫斯基距离(Minkowski distance)的定义 ，即$L_p$范数，当p=2时即为我们常见的欧氏距离，当p=1时即为曼哈顿距离。$$L_p(x_i,x_j)=(\\displaystyle\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$$当然也有其他的距离度量的选择，如马氏距离(Mahalanobis distance)，余弦距离(Cosine distance)、杰卡德距离(Jaccard distance)、相关距离(Correlation distance)和信息熵(Information Entropy) 等，有兴趣的话可以自己去查阅资料，这里不过多赘述。k值的选择也很重要，这会对k近邻的结果产生重大的影响。 如果选择较小的K值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有输入实例较近的训练实例才会对预测结果起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值得减小就意味着整体模型非常复杂，容易发生过拟合。如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测，其实有点像是在减少学习的估计误差，但缺点是学习的近似误差会增大。这时与输入实例较远的训练实例也会起预测作用，使预测发生错误，k值得增大就意味着整体的模型变得简单。如果K=N，那么无论输入实例是什么，都将简单的预测它属于训练实例中最多达到类。这时，模型过于简单，完全忽略训练中的大量有用信息，是不可取的。在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选择最优k值。按经验规则来说：k一般低于训练样本数的平方根 。 3 分类决策规则k近邻的分类决策规则是最为常见的多数投票规则，也就是在最近的K个点中，哪个标签数目最多，就把目标点的标签归于哪一类。当然，这也是有理论依据的：如果分类的损失函数为0-1损失函数，则误分类的概率是：$$P(Y≠f(X))=1-P(Y=f(X))$$也就是说误分类率为：$$\\frac{1}{k}\\sum I(y_i≠c_j)=1-\\frac{1}{k}I(y_i=c_j)$$要使得误分类率最小，也就是经验风险最小，就要使得$\\frac{1}{k}I(y_i=c_j)$最大，所以多数表决规则等价于经验最小化。 4 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import numpy as npfrom collections import Counterfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import normalizeimport mathnp.random.seed(1024)def euclidean_distance(x1, x2): \"\"\" Calculates the l2 distance between two vectors \"\"\" distance = 0 for i in range(len(x1)): distance += pow((x1[i] - x2[i]), 2) return math.sqrt(distance)class KNN: def __init__(self, k=3): self.k = k # @staticmethod # def _majority_vote(n_neighbor_labels): # return np.argmax(np.bincount(n_neighbor_labels)) @staticmethod def _majority_vote(n_neighbor_labels): cnt = Counter(n_neighbor_labels) return cnt.most_common(1)[0][0] def predict(self, X_train, y_train, X_test): y_preds = [] for x_t in X_test: distances = [euclidean_distance(x_t, x) for x in X_train] inds = np.argsort(distances)[:self.k] y_label = self._majority_vote([y_train[i] for i in inds]) y_preds.append(y_label) return y_preds# def toy_data():# # create a matrix: each row as a sample# feature = np.array([[1.0, 0.9], [1.0, 1.0], [0.1, 0.2], [0.0, 0.1]])# labels = ['a', 'a', 'b', 'b']# return feature, labelsdef accuracy_score(y_true, y_pred): accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true) return accuracyif __name__ == '__main__': data = datasets.load_iris() X = normalize(data.data) y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33) knn = KNN(5) y_pred = knn.predict(X_train, y_train, X_test) acc = accuracy_score(y_pred, y_test) print(acc) 参考资料《统计学习方法》","tags":[{"name":"算法","slug":"算法","permalink":"http://tpioneer.github.io/tags/算法/"},{"name":"笔记","slug":"笔记","permalink":"http://tpioneer.github.io/tags/笔记/"}]},{"title":"机器学习算法总结二——线性回归","date":"2018-05-20T12:10:24.000Z","path":"2018/05/20/2018-05-13-机器学习算法总结二——线性回归/","text":"关键词：线性回归 平方误差函数 梯度下降 1 线性回归模型简介首先给出回归问题的一个定义：回归问题指的是在给定D维输⼊（input）变量x的情况下，预测⼀个或者多个连续目标（target）变量t的值。最常见的应用就是各种曲线拟合的场景，例如对一段时间房屋价格进行预测就是一个典型的回归问题。线性回归模型的最简单的形式就是目标值y是输入变量x的线性函数。但是，通过将⼀组输入变量的非线性函数进行线性组合，我们可以得到一类更有用的函数，被称为基函数（basis function）。这样的模型是参数的线性函数，这使得其分析起来较为简单，同时对于输入变量来说模型是非线性的。给定⼀个由N个观测值 $\\{x_n\\}$组成的数据集，其中$n = 1,…N$，以及对应的目标值${t_n}$，我们的目标是预测对于给定新的$x$值的情况下$t$的值。最简单的方法是，直接建立⼀个适当的函数y(x)，对于新的输入$x$，这个函数能够直接给出对应的$t$的预测。更⼀般地，从⼀个概率的观点来看，我们的目标是对预测分布$p(t | x)$建模，因为它表达了对于每个$x$值，我们对于t的值的不确定性。从这个条件概率分布中，对于任意的$x$的新值，我们可以对$t$进行预测，这种方法等同于最小化⼀个恰当选择的损失函数的期望值。 回归模型的最简单形式如下：$$y(x,w) = w_0 + w_1x_1 + \\cdots +w_dx_d \\tag{1}$$其中$x = (x_1,…, x_D)^T$ 。这通常被简单地称为线性回归（linear regression）。这个模型的关键性质是它是参数$w_0,…,w_D$的⼀个线性函数。但是，它也是输入变量$x_i$的⼀个线性函数，这给模型带来的极大的局限性。因此我们这样扩展模型的类别：将输入变量的固定的非线性函数进行线性组合，形式为$$y(x,w) = \\sum_{j=0}^{M-1}w_j\\phi_j(x) = w^T\\phi(x) \\tag{2}$$其中$\\phi_j(x)$被称为基函数（basis function），$w = (w_0,…,w_{M-1})^T$ 且$ϕ = (\\phi_0,…, \\phi_{M-1})^T$ 。基函数的选择有很多，如常见的高斯基函数,其中$\\mu_j$控制了基函数在输⼊空间中的位置，参数s控制了基函数的空间大小。：$$\\phi_j(x) = \\exp\\bigg\\{-\\frac{(x-\\mu_j)^2}{2s^2}\\bigg\\} \\tag{3}$$sigmoid基函数:$$\\phi_j(x) = \\sigma\\Big(\\frac{x-\\mu_j}{s}\\Big) \\tag{4}$$其中$\\sigma(a)$是logistic sigmoid函数，定义为$$\\sigma_a = \\frac{1}{1+\\exp(-a)} \\tag{5}$$但是在这里本文几乎不会涉及到基函数的选择，这里默认$\\phi(x)=x$。 2 最大似然与最小平方我们假设目标变量$t$由确定的函数$y(x,w)$给出，这个函数被附加了高斯噪声，即$$t = y(x,w) + \\epsilon \\tag{6}$$其中$ϵ$是⼀个零均值的高斯随机变量，精度（方差的倒数）为$\\beta$ 。因此我们有$$p(t|x,w,\\beta) = \\mathcal{N}(t|y(x,w),\\beta^{-1}) \\tag{7}$$接着考虑这样一个数据集$X = (x_1,…, x_N)$，对应的目标值是$t_1,…,t_N$，记作向量$\\boldsymbol{t}$假设这些数据点是独立地从分布(7)中抽取那么我们可以得到下面的似然函数的表达式，它是可调节参数$w$和$\\beta$ 的函数，形式为$$p(t|X,w,\\beta) = \\prod\\mathcal{N}(t_n|w^T\\phi(x_n),\\beta^{-1}) \\tag{8}$$按照⼀元高斯分布的标准形式对(8)式取对数可得对数似然函数如下：$$\\begin{align}\\mathrm{ln}p(t|w,\\beta) &amp;= \\sum_{n=1}^{N}\\mathrm{ln}\\mathcal{N}(t_n|w^T\\phi(x_n), \\beta^-1) \\\\&amp;= \\frac{N}{2}\\mathrm{ln}\\beta - \\frac{N}{2}\\mathrm{ln}(2\\pi) - \\beta E_D(w)\\tag{9}\\end{align}$$其中平方和误差函数$E_D(w)$的定义为$$E_D(w) = \\frac{1}{2}\\sum_{n=1}^N \\{tn-w^T\\phi(x_n)\\}^2 \\tag {10}$$由(9)式可以发现，由于右式前两项与$w$无关，则最大化似然函数在高斯噪声的情况下等价于最小化平方误差函数$E_D(w)$,令(9)式的梯度为0有$$0 = \\sum_{n=1}^Nt_n\\phi(x_n)^T - w^T(\\sum_{n=1}^N\\phi(x_n)\\phi(x_n)^T) \\tag{11}$$求解$w$，我们有$$w_{ML} = (\\Phi^T\\Phi)^{-1}\\Phi^Tt \\tag{12}$$这被称为最小平方问题的规范方程（normal equation）。这里$\\Phi$是⼀个N$\\times$M的矩阵，被称为设计矩阵（design matrix），它的元素为$\\Phi_{nj} = \\Phi_j(x_n)$，即$$\\Phi = \\left( \\begin{array}\\\\phi_0(x_{1}) &amp; \\phi_0(x_{1}) &amp; \\ldots &amp;\\phi_{M-1}(x_{1}) \\\\\\phi_0(x_{2}) &amp; \\phi_1(x_{2}) &amp; \\ldots &amp; \\phi_{M-1}(x_{2}) \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots \\\\\\phi_0(x_{N}) &amp;\\phi_1(x_{N})&amp; \\ldots &amp;\\phi_{M-1}(x_{N})\\end{array} \\right) \\tag{13}$$ 令$$\\Phi^† \\equiv (\\Phi^T\\Phi)^{-1}\\Phi^T \\tag{14}$$$\\Phi^†$被称为矩阵$\\Phi$的Moore-Penrose伪逆矩阵（pseudo-inverse matrix）。 3 顺序学习最大似然解(12)的求解过程涉及到⼀次处理整个数据集。这种批处理技术对于大规模数据集来说计算量相当大。正如我们在第1章讨论的那样，如果数据集充分大，那么使用顺序算法（也被称为在线算法）可能更有价值。顺序算法中，每次只考虑⼀个数据点，模型的参数在每观测到⼀个数据点之后进行更新。顺序学习也适用于实时的应⽤。在实时应用中，数据观测以⼀个连续的流的⽅式持续到达，我们必须在观测到所有数据之前就做出预测。我们可以获得⼀个顺序学习的算法通过考虑随机梯度下降（stochastic gradient descent），也被称为顺序梯度下降（sequential gradient descent）的⽅法。如果误差函数由数据点的和组成$E =\\sum_n E_n$，那么在得到第n组数据之后，随机梯度下降算法使用下式更新参数向量w$$w^{(\\tau +1)} = w^{(\\tau)} - \\eta\\nabla E_n \\tag{15}$$其中$\\tau$表示迭代次数，$\\eta$是学习率参数。我们稍后会讨论$\\eta$的选择问题。$w$被初始化为某个起始向量$w^{(0)}$。对于平方和误差函数（3.12）的情形，我们有$$w^{(\\tau +1)} = w^{(\\tau)} +\\eta(t_n - w^{(\\tau) T} \\phi_n)\\phi_n \\tag{16}$$其中$\\phi_n=\\phi(x_n)$。 4 算法实现下面附上线性回归的一个简单实现，这里考虑到可能会扩展到带有正则项的线性回归，将梯度下降法的实现放在了基类Regression中，在LinearRegression中则实现了线性回归的闭式解，即通过计算$\\Phi$的伪逆矩阵来直接得到$w$的解。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Regression(object): \"\"\" n_iterations: float The number of training iterations. learning_rate: float The step length used for updating the weights. \"\"\" def __init__(self, n_iterations, learning_rate): self.n_iterations = n_iterations self.learning_rate = learning_rate def initialize_weights(self, n_features): \"\"\" Initialize weights randomly [-1/N, 1/N] \"\"\" limit = 1 / math.sqrt(n_features) self.w = np.random.uniform(-limit, limit, (n_features, )) def fit(self, X, y): # Insert constant ones for bias weights X = np.insert(X, 0, 1, axis=1) self.training_errors = [] self.initialize_weights(n_features=X.shape[1]) # Do gradient descent for n_iterations for i in range(self.n_iterations): y_pred = X.dot(self.w) # Calculate l2 loss mse = np.mean(0.5 * (y - y_pred)**2 self.training_errors.append(mse) # with equation (16) # Gradient of l2 loss w.r.t w grad_w = -(y - y_pred).dot(X) # Update the weights self.w -= self.learning_rate * grad_w def predict(self, X): # Insert constant ones for bias weights X = np.insert(X, 0, 1, axis=1) y_pred = X.dot(self.w) return y_pred class LinearRegression(Regression): def __init__(self, n_iterations=100, learning_rate=0.001, gradient_descent=False): self.gradient_descent = gradient_descent super(LinearRegression, self).__init__(n_iterations=n_iterations, learning_rate=learning_rate) def fit(self, X, y): # If not gradient descent =&gt; Least squares approximation of w if not self.gradient_descent: # Insert constant ones for bias weights X = np.insert(X, 0, 1, axis=1) # see the equation (14) self.w = np.linalg.pinv(X).dot(y) else: super(LinearRegression, self).fit(X, y) 参考资料 《Pattern Recognition and Machine Learning》 Christopher M Bishop","tags":[{"name":"算法","slug":"算法","permalink":"http://tpioneer.github.io/tags/算法/"},{"name":"笔记","slug":"笔记","permalink":"http://tpioneer.github.io/tags/笔记/"}]},{"title":"机器学习算法总结一——感知机","date":"2018-05-13T11:33:56.000Z","path":"2018/05/13/机器学习算法总结一——感知机/","text":"关键词：感知机 对偶形式 判别模型 1 感知机模型感知机（perceptron）是二类分类的简单线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1为二值，感知机对应于输入空间（特征空间）中将实例分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机模型的数学表达式如下:$$f(x)=sign(w \\cdot x+b)$$其中，w和为感知机模型参数，$w$叫作权值，b叫作偏置，$w \\cdot x$表示$w$和$x$的内积,sign是符号函数，即$$sign(x)= \\begin{cases} -1&amp; {x&lt;0}\\\\ 1&amp; {x\\geq 0} \\end{cases}$$ 感知机模型的假设空间是定义在特征空间中的所有线性分类模型，即函数集合$$\\{f|f(x)=w \\cdot x +b\\}$$这样在学习的过程中通过对训练数据集的学习确定参数w和b，然后通过学习到的感知机模型去预测新的输入实例。 2 感知机学习策略为了找出分离超平面，我们需要确定感知机模型参数$w$和b，所以需要一个有效的学习策略，即定义损失函数并将其极小化。那么很容易想到的一个损失函数的选择是误分类点的总数，但是，这样的参数不是$w$和b的连续可导函数，不容易优化。感知机学习实际上采用的是误分类点到超平面S的总距离来作为损失函数，定义如下：$$\\frac{1}{||w||} \\left|w \\cdot x_0 + b \\right|$$这里$||w||$是$w$的$L_2$范数。其次，对于误分类的数据$-y_i(w \\cdot x_i +b)&gt;0$成立，因为当$w \\cdot x_i +b &gt;0$时，$y_i=-1$,而$w \\cdot x_i +b&lt;0$时，$y_i=+1$，因此，误分类点$x_i$到超平面S的距离是$$-\\frac{1}{||w||} y_i \\left(w \\cdot x_i + b \\right)$$这样，假设误分类点的集合为M，那么总距离为：$$-\\frac{1}{||w||}\\sum\\limits_{x_i \\in M} y_i \\left(w \\cdot x_i + b \\right)$$这样我们就得到了初步的感知机模型的损失函数。我们研究可以发现，分子和分母都含有w,当分子的w扩大N倍时，分母的$L_2$范数也会扩大N倍。也就是说，可以不考虑$\\frac{1}{||w||}$。最终感知机模型的损失函数简化为：$$L(w,b) = -\\sum\\limits_{x_i \\in M} y_i \\left(w \\cdot x_i + b \\right)$$显然，损失函数$L（w,b）$是非负的，如果分类正确，即没有误分类点，则损失函数值为0，而且，误分类点聚类分类超平面越近，损失函数值越小。 3 感知机学习算法3.1 感知机学习算法的原始形式用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,可以采用随机梯度下降来学习模型。在任意选定一个超平面后，我们每次仅仅需要随机选取一个误分类的点来更新梯度。损失函数基于w和b参数的偏导数分别为：$$\\begin{aligned}\\nabla_w L(w,b) &amp;= -\\sum\\limits_{x_i \\in M} y_i x_i\\\\\\nabla_b L(w,b) &amp;= -\\sum\\limits_{x_i \\in M} y_i\\end{aligned}$$w,b的梯度下降迭代公式应该为：$$\\begin{aligned}w &amp;= w + \\eta y_i x_i\\\\\\b &amp;= b + \\eta y_i\\end{aligned}$$其中，$\\eta$叫作学习率（learning rate）。这样通过迭代可以让损失函数不断减小，直到为0。那么算法的步骤总结如下：(1) 选取初值$w_0,b_0$,一般取为0(2) 在训练集中选取数据$(x_i,y_i)$(3) 如果$y_i(w \\cdot x_i + b)\\leq 0$,$$\\begin{align}w &amp;= w + \\eta y_ix_i \\\\\\b &amp;= b + \\eta y_i\\end{align}$$(4) 转至（2），直至训练集中没有误分类点。下面将偏置b并入权重向量w, 记作$\\hat{w}=(w^T,b)^T$,相应的，$\\hat{x}=(x^T,1)^T$。显然$\\hat{w}\\cdot \\hat{x} = w \\cdot x +b$. 3.2 感知机学习算法的对偶形式对偶形式的主要目的是简化计算，其基本思想是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得参数。我们假设初始值均为0，误分类样本点$(x_i,y_i)$在更新过程中修改了n次，那么由上述的学习过程不难得到：$$\\begin{align}w &amp;= \\sum_{i=1}^Nn_i \\eta y_ix_i \\\\b &amp;= \\sum_{i=1}^Nn_i\\eta y_i\\end{align}$$当$\\eta=1$时，表示第$i$个实例点由于误分而进行更新的次数，实例点更新次数越多，意味着它距离分离超平面越近，在SVM中这样的点也越有可能是支持向量。将上式带入原始形式，令$\\alpha_i = n_i\\eta$, 则新的感知机模型为$$f(x) = sign\\{\\sum_{j=1}^N\\alpha_j y_jx_jx+b \\}.$$类比原始形式的感知机学习算法，对偶形式算法如下：(1) $\\alpha=0, b=0$(2) 在训练集中选取数据$(x_i,y_i)$(3) 如果$y_i(\\sum_{j=1}^N\\alpha_j y_jx_jx+b)\\leq 0$,$$\\begin{align}\\alpha_i &amp;= \\alpha_i+ \\eta \\\\b &amp;= b + \\eta y_i\\end{align}$$(4) 转至（2），直至训练集中没有误分类点。对偶形式的意义在于在对偶形式中训练实例仅以内积的形式出现，所以如果提前将训练集中实例间的内积计算出来并以矩阵的形式存储，将大大加快计算速率。这个矩阵就是所谓的Gram矩阵。$$G = [x_i \\cdot x_j]_{N\\times N}$$ 4 感知机的简单实现下面给出了感知机的简单python实现，分别实现了原始形式和对偶形式的感知机模型。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import numpy as npclass Perceptron: \"\"\" Perceptron classifier. Parameters: ----------- learning_rate: int \"\"\" def __init__(self, learning_rate=1): self.learning_rate = learning_rate # 原始形式 def fit(self, X, y): n_samples, n_features = np.shape(X) self.W = np.zeros(n_features) self.w0 = 0 flag = True while flag: flag = False for i in range(n_samples): # Calculate outputs linear_output = X[i].dot(self.W) + self.w0 if y[i]*linear_output &lt;= 0: flag = True self.W += self.learning_rate*y[i]*X[i] self.w0 += self.learning_rate*y[i] print(self.W, self.w0) # 对偶形式 def fit_dual(self, X, y): n_samples, n_features = np.shape(X) a = np.zeros(n_samples) b = 0 matrix = self.make_gram_matrix(X, n_samples) flag = True while flag: flag = False for i in range(n_samples): # Calculate outputs linear_output = sum([a[j] * y[j] * matrix[j][i] for j in range(n_samples)]) + b if y[i]*linear_output &lt;= 0: flag = True a[i] += self.learning_rate b += self.learning_rate*y[i] print(a,b) self.W = sum([a[i]*y[i]*X[i] for i in range(n_samples)]) self.w0 = sum([a[i]*y[i] for i in range(n_samples)]) @staticmethod def make_gram_matrix(X, n_features): matrix = np.array([X[i].dot(X[j]) for i in range(n_features) for j in range(n_features)]).reshape(n_features, n_features) return matrix # Use the trained model to predict labels of X def predict(self, x): print(\"y = &#123;&#125;x + &#123;&#125;\".format(self.W, self.w0)) y_pred = (x.dot(self.W) + self.w0) return 1 if y_pred &gt; 0 else 0 if __name__ == '__main__': per = Perceptron() train_x = np.array([[3, 3], [4, 3], [1, 1]]) train_y = np.array([1, 1, -1]) per.fit(train_x, train_y) print(per.predict(np.array([2, 3]))) per.fit_dual(train_x, train_y) print(per.predict(np.array([2, 3]))) 参考文献李航 《统计学习方法》 第二章","tags":[{"name":"算法","slug":"算法","permalink":"http://tpioneer.github.io/tags/算法/"},{"name":"笔记","slug":"笔记","permalink":"http://tpioneer.github.io/tags/笔记/"}]},{"title":"HEXO搭建你的专属博客","date":"2018-05-11T15:33:56.000Z","path":"2018/05/11/HEXO搭建你的专属博客/","text":"关键词：github hexo blog STEP1 前期准备 在这里默认你已经拥有了github账号，如果没有，还等什么，赶快加入最大的同性交友平台吧（雾。 在github中创建一个名为username.github.io（这里的username即你的账号名，比如我的为Tpioneer.github.io）的库，创建完 后可以试着往库中push一个简单的index.html如下。（github的简单操作可以参考如下教程） &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;h1&gt;Hello World&lt;/h1&gt; &lt;p&gt;I'm hosted with GitHub Pages.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 接着在你的浏览器中输入https://username.github.io/（username为你的账号名），如果没什么问题的话即可访问到Hello World。 同时，需要安装git，可以去官网下载。 STEP2 hexo环境配置与安装 hexo是一款基于Node.js的静态博客框架, 在安装hexo之前，我们需要先安装node.js,这里对安装不做过多介绍，官网的文档已经很详细了。 Node和Git都安装好后,首先创建一个blog文件夹,然后进入blog目录来安装Hexo（windows用户推荐用Git Bash）。执行如下命令安装Hexo： sudo npm install -g hexo 安装完成后，执行init命令初始化hexo,命令： hexo init然后等待安装完成，即完成了hexo的安装。 安装完成后需要对hexo进行配置，找到/hexo/_config.yml文件，用任意编辑器打开该文件，拉到最后找到Deployment,将其改为下图所示内容： 这里有个需要注意的点，在冒号后面一定要空格，否则识别不出来 修改头像，同样需要打开_config.yml ，找到 avatar: 这一行，然后添加头像的URL就行了，当然你也可以将头像图片放入hexo/source/img目录下，然后将配置改为如下：avatar: /img/xxx.jpg 同理可以修改你的博客名，博客描述，作者等信息，这里发一张我的参考配置如下： 当然配置文件中还有很多其他的配置选项，在这里就不一一赘述了，官网上提供了更详细的参考资料 STEP3 hexo的部署和常用命令 部署步骤如下(建议在每次重新部署时都执行一次hexo clean)：hexo cleanhexo generate (hexo g)hexo deploy (hexo d)在部署完成后，在你的浏览器中输入https://username.github.io/，大功告成，撒花★,°:.☆(￣▽￣)/$:.°★ 。 当然，这只是简单的部署了属于你自己的一个blog，你还需要稍微再花费点时间让你的blog更加丰富多彩。 首先，你需要了解下面提供的一些常用命令： hexo new “postName” #新建文章 hexo new page”pageName” #新建页面 hexo generate #生成静态页面至public目录 hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server） hexo deploy #将.deploy目录部署到GitHub hexo help #查看帮助 hexo version #查看Hexo的版本 了解了这些命令，相信你写一些简单的blog已经没有什么问题了，但是中庸的官方主题肯定不会让你特别喜欢。这里有个主题集合,你可以在这里尽情挑选你喜欢的主题，每个主题的github中都有相应的文档，所以，大胆的去尝试吧，相信你一定能找到适合你的那个。 最后，送上一份Markdown的语法说明书。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://tpioneer.github.io/tags/随笔/"}]}]